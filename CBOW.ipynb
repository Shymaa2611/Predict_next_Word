{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125975cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "folder_path = 'D:\\\\MachineCourse\\\\NLP_Course\\\\Tasks\\\\CBOW\\\\dataset'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "count = 1\n",
    "with open('D:\\\\MachineCourse\\\\NLP_Course\\\\Tasks\\\\CBOW\\\\people_wiki.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        URI,name, text = row\n",
    "        with open(os.path.join(folder_path, f'doc{count}.txt'), 'w') as text_file:\n",
    "            text_file.write(text)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f38c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073b3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    html_pattern = r'<.*?>'\n",
    "    without_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
    "    return without_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b26557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "    return without_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c06176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494415eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=remove_html_tags(text)\n",
    "    text=convert_to_lower(text)\n",
    "    text=remove_urls(text)\n",
    "    text=remove_numbers(text)\n",
    "    #text=remove_stopwords(text)\n",
    "    text=remove_extra_white_spaces(text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = 'D:\\\\MachineCourse\\\\NLP_Course\\\\Tasks\\\\CBOW\\\\dataset'\n",
    "def read_text_files(folder_path):\n",
    "    all_words = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text=clean(text)\n",
    "                words=text.split()\n",
    "                all_words.append(words)\n",
    "    return all_words\n",
    "words = read_text_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c4fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773277ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10721318",
   "metadata": {},
   "source": [
    "Split data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979cd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x, y in data]\n",
    "y = [y for x, y in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c145545",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([word for sentence in X for word in sentence] + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f013f",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc333fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is incorrectly structured as lists of lists, fix the structure\n",
    "X_fixed = [' '.join(map(str, context)) for context in X]\n",
    "\n",
    "X_encoded = tokenizer.texts_to_sequences(X_fixed)\n",
    "y_encoded = tokenizer.texts_to_sequences(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a8173",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_padded = pad_sequences(X_encoded, maxlen=4, padding='post')\n",
    "y_padded = pad_sequences(y_encoded, maxlen=1, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca17e3",
   "metadata": {},
   "source": [
    "Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_padded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cf6ff",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=4))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history=model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a788d",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "model.save('checkpoint.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88525886",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad84daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "acc=history_dict['accuracy']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "plt.clf()\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba66789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, context_str, top_n=3):\n",
    "    cleaned_context = clean(context_str)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_context])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=4, padding='post')\n",
    "    preds = model.predict(padded_sequence, verbose=0)[0]\n",
    "    top_indices = preds.argsort()[-top_n:][::-1]\n",
    "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    top_words = [index_word[idx] for idx in top_indices if idx in index_word]\n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('checkpoint.h5')\n",
    "context_example_loaded = \"he go to \"\n",
    "predicted_words_loaded = predict_next_word(loaded_model, tokenizer, context_example_loaded)\n",
    "print(\"Predicted words with loaded model:\", predicted_words_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c26ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9511f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9882ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
